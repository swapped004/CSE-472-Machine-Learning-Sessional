{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dpC0WIcOj_r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZyKAvQGxkCi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Setup\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kp28RcVyWUB"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qa_SWmCye79"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui-PZ0Apy40T"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkj-2PqOy9-R"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwJZ4xvwzFTM"
      },
      "outputs": [],
      "source": [
        "#Data Handling\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SA0NpOFzMfp"
      },
      "outputs": [],
      "source": [
        "class banglaDataset():\n",
        "  def loadData(self):\n",
        "    json_path = \"/content/drive/MyDrive/caption_project/datasets/captions.json\"\n",
        "\n",
        "    f = open(json_path, encoding = \"utf-8\")\n",
        "\n",
        "    images = []\n",
        "    captions = []\n",
        "\n",
        "    json_f = json.load(f)\n",
        "    for d in tqdm(json_f):\n",
        "        # print(d)\n",
        "        # print(d['filename'])\n",
        "\n",
        "        #read image\n",
        "        img_file = \"/content/drive/MyDrive/caption_project/datasets/images/\" + d['filename']\n",
        "        img = cv2.imread(img_file)\n",
        "        if img is None:\n",
        "            print(\"image is None\")\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "\n",
        "        images.append(img)\n",
        "\n",
        "        caption = d['caption']\n",
        "        captions.append(caption)\n",
        "    \n",
        "\n",
        "    #convert to numpy array\n",
        "    images = np.array(images)\n",
        "    captions = np.array(captions)\n",
        "\n",
        "    print(images.shape)\n",
        "    print(captions.shape)\n",
        "\n",
        "    return images, captions\n",
        "\n",
        "  def createFnameCaptionMap(self):\n",
        "    json_path = \"/content/drive/MyDrive/caption_project/datasets/captions.json\"\n",
        "\n",
        "    f = open(json_path, encoding = \"utf-8\")\n",
        "\n",
        "    json_f = json.load(f)\n",
        "    cap_dict = collections.defaultdict(list)\n",
        "\n",
        "    for d in tqdm(json_f):\n",
        "      f_name = \"/content/drive/MyDrive/caption_project/datasets/images/\" + d['filename']\n",
        "      captions = d['caption']\n",
        "      cap_dict[f_name] = captions\n",
        "\n",
        "    # print(cap_dict)\n",
        "\n",
        "    return cap_dict\n",
        "\n",
        "  def splitData(self, cap_dict, test_size = 0.1):\n",
        "    json_path = \"/content/drive/MyDrive/caption_project/datasets/captions.json\"\n",
        "    f = open(json_path, encoding = \"utf-8\")\n",
        "    json_f = json.load(f)\n",
        "\n",
        "    train_files = []\n",
        "    test_files = []\n",
        "\n",
        "    train_size = 9154*(1-test_size)\n",
        "\n",
        "    i = 0\n",
        "    for d in tqdm(json_f):\n",
        "      f_name = d['filename']\n",
        "      if i <= train_size:\n",
        "        train_files.append(f_name)\n",
        "      else:\n",
        "        test_files.append(f_name)\n",
        "      i+=1\n",
        "\n",
        "    # print(train_files)\n",
        "\n",
        "    train_captions = [(str(\"/content/drive/MyDrive/caption_project/datasets/images/\"+f_name), cap_dict[str(\"/content/drive/MyDrive/caption_project/datasets/images/\"+f_name)]) for f_name in train_files]\n",
        "    test_captions = [(str(\"/content/drive/MyDrive/caption_project/datasets/images/\"+f_name), cap_dict[str(\"/content/drive/MyDrive/caption_project/datasets/images/\"+f_name)]) for f_name in test_files]\n",
        "\n",
        "    train_ds = tf.data.experimental.from_list(train_captions)\n",
        "    test_ds = tf.data.experimental.from_list(test_captions)\n",
        "\n",
        "    \n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL6E2tLWZTbs"
      },
      "source": [
        "Flickr8K Bangla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jef7inj1ZVii"
      },
      "outputs": [],
      "source": [
        "class flickr8kBangla():\n",
        "  def createFnameCaptionMap(self):\n",
        "    csv_path = \"/content/drive/MyDrive/caption_project/datasets/BAN-Cap_captiondata.csv\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    #extract the image paths and captions\n",
        "    image_paths = df['caption_id'].tolist()\n",
        "    captions = df['bengali_caption'].tolist()\n",
        "\n",
        "    cap_dict  = collections.defaultdict(list)\n",
        "\n",
        "    for i in range(len(image_paths)):\n",
        "        image_name = image_paths[i].split('#')[0]\n",
        "        cap_dict[image_name].append(captions[i])\n",
        "\n",
        "    return cap_dict\n",
        "\n",
        "\n",
        "  def splitData(self, cap_dict, test_size = 0.1):\n",
        "    csv_path = \"/content/drive/MyDrive/caption_project/datasets/BAN-Cap_captiondata.csv\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    #extract the image paths and captions\n",
        "    image_paths = df['caption_id'].tolist()\n",
        "    captions = df['bengali_caption'].tolist()\n",
        "\n",
        "    image_names = list(set([i.split('#')[0] for i in image_paths]))\n",
        "\n",
        "    train_files = []\n",
        "    test_files = []\n",
        "\n",
        "    train_size = 8090*(1-test_size)\n",
        "\n",
        "    i=0\n",
        "\n",
        "    for d in tqdm(image_names):\n",
        "      if i <= train_size:\n",
        "        train_files.append(d)\n",
        "      else:\n",
        "        test_files.append(d)\n",
        "\n",
        "      i+=1\n",
        "\n",
        "    train_captions = [(str(\"/content/drive/MyDrive/flickr_Images/\"+f_name), cap_dict[f_name]) for f_name in train_files]\n",
        "    test_captions = [(str(\"/content/drive/MyDrive/flickr_Images/\"+f_name), cap_dict[f_name]) for f_name in test_files]\n",
        "\n",
        "    print(test_captions)\n",
        "\n",
        "    train_ds = tf.data.experimental.from_list(train_captions)\n",
        "    test_ds = tf.data.experimental.from_list(test_captions)\n",
        "\n",
        "    \n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoy-b55Qzn6W"
      },
      "outputs": [],
      "source": [
        "bd = banglaDataset()\n",
        "bd_images, bd_captions = bd.loadData()\n",
        "print(bd_images.shape)\n",
        "print(bd_captions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXl4cgYzzqCg"
      },
      "outputs": [],
      "source": [
        "#Save in a picle \n",
        "pickle_path = \"/content/drive/MyDrive/caption_project/pickles/bangla_data.pkl\"\n",
        "with open(pickle_path, \"wb\") as f:\n",
        "        pickle.dump((bd_images, bd_captions), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HBbfBGWzu2M"
      },
      "outputs": [],
      "source": [
        "bd = flickr8kBangla()\n",
        "cap_dict = bd.createFnameCaptionMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD7-MaG3z04D"
      },
      "outputs": [],
      "source": [
        "train_captions, test_captions = bd.splitData(cap_dict)\n",
        "print(type(train_captions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oolb4juK0CK9"
      },
      "outputs": [],
      "source": [
        "choose = 'bangla'\n",
        "if choose == 'bangla':\n",
        "  # train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)\n",
        "  bd = banglaDataset()\n",
        "  cap_dict = bd.createFnameCaptionMap()\n",
        "  train_raw, test_raw = bd.splitData(cap_dict)\n",
        "\n",
        "elif choose == 'flickr8k_bangla':\n",
        "  bd = flickr8kBangla()\n",
        "  cap_dict = bd.createFnameCaptionMap()\n",
        "  train_raw, test_raw = bd.splitData(cap_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acKC5zea0NpZ"
      },
      "outputs": [],
      "source": [
        "train_raw.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40N3SL3E0VEK"
      },
      "outputs": [],
      "source": [
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiM4FOBI0YnT"
      },
      "outputs": [],
      "source": [
        "#Extract Image Features\n",
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True)\n",
        "mobilenet.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcY9wL4i0e0x"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgCsr44_0i6K"
      },
      "outputs": [],
      "source": [
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfZ_iWq0lqG"
      },
      "outputs": [],
      "source": [
        "def standardize(s):\n",
        "  # s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGnRM7tV0n4r"
      },
      "outputs": [],
      "source": [
        "# Use the top 5000 words for a vocabulary.\n",
        "vocabulary_size = 10000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAODu-t60x7v"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7W04eYs03Wj"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l80r69c4Ln_0"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odp906WUOmHj"
      },
      "outputs": [],
      "source": [
        "#The train_raw and test_raw datasets contain 1:many (image, captions) pairs.\n",
        "\n",
        "#This function will replicate the image so there are 1:1 images to captions:\n",
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuWUY7ZbOqVQ"
      },
      "outputs": [],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFhFVPrTO2ZA"
      },
      "outputs": [],
      "source": [
        "#To be compatible with keras training the dataset should contain (inputs, labels) pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an (images, texts) pair to an ((images, input_tokens), label_tokens) pair:\n",
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUnA19mrPB9e"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zcg24NZXPIGv"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHkU_9w6PKqM"
      },
      "outputs": [],
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qwziv6DP7JM"
      },
      "outputs": [],
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWKOi--j-QnQ"
      },
      "outputs": [],
      "source": [
        "print(ex_img[0])\n",
        "print(ex_in_tok[0])\n",
        "print(ex_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By4i6XLLT7_h"
      },
      "outputs": [],
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        mask_zero=True)\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuKn2ib8GUsJ"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OPahOizGZ8Y"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhpYI6QIGh3P"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6zuUcJ_HFTA"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP5cVflAHMx-"
      },
      "outputs": [],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t1bwtd8MrKR"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        depth=units,\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_BOiFKvM3ef"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def call(self, inputs):\n",
        "  image, txt = inputs\n",
        "\n",
        "  if image.shape[-1] == 3:\n",
        "    # Apply the feature-extractor, if you get an RGB image.\n",
        "    image = self.feature_extractor(image)\n",
        "  \n",
        "  # Flatten the feature map\n",
        "  image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "  if txt.dtype == tf.string:\n",
        "    # Apply the tokenizer if you get string inputs.\n",
        "    txt = tokenizer(txt)\n",
        "\n",
        "  txt = self.seq_embedding(txt)\n",
        "\n",
        "  # Look at the image\n",
        "  for dec_layer in self.decoder_layers:\n",
        "    txt = dec_layer(inputs=(image, txt))\n",
        "    \n",
        "  txt = self.output_layer(txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkn8SnY2M8m4"
      },
      "outputs": [],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVdDsxKaNAT0"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLQex0rWNQXT"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8BaeT6wNTML"
      },
      "outputs": [],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M788w8_sN55k"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z95zMvYqNuM1"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssK_e-LpN-Ln"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQl1ZReaOA5e"
      },
      "outputs": [],
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moy0MY7-OGUB"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho2sTvndOLTE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHnvsJZVOR6O"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO7N4z4XOYx9"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnjjJkCgOcWg"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyQsZp8qfAOe"
      },
      "outputs": [],
      "source": [
        "result = model.simple_gen(image, temperature=0.5)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVohO9MbfDa2"
      },
      "outputs": [],
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIlWcKJmfI04"
      },
      "outputs": [],
      "source": [
        "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "[map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LxvgwVvfOFa"
      },
      "outputs": [],
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence (height width) -> sequence height width',\n",
        "    height=7, width=7,\n",
        "    reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usd_MNskfQyH"
      },
      "outputs": [],
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-RsNyJfS3r"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "    \n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2s73D5TfVJw"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpG3RBIPfd5f"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  print(result_txt)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "  \n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5RzBTiFfgyz"
      },
      "outputs": [],
      "source": [
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ivFkstGsfnoQ"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/caption_project/datasets/images/6946.png'\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}